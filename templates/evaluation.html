<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>model_evaluation</title>
      <style>
        table, th, td {
  border: 1px solid black;
}
    </style>
</head>
<body>
<h2>Machine Learning Model Evaluation</h2>
For the Data Analysis of the given dataset, following are the two ML algorithms that were implemented :
<ul>
  <li>Gaussian Naïve-Bayes </li>
  <li>Logistic Regression</li>
</ul>
<h3>Rationale for Model Selection : </h3>
The Gaussian Naïve-Bayes algorithm is based on the principle of <b>Conditional Probability</b> where as Logistic Regression is
based on the principle of, as the name itself says, <b>Regression Analysis</b>. The Logistic regression is a type of Regression that
works over Classification problem, something that is needed in this dataset since, we have to classify between fraudulent or not fraudulent.
The Gaussian Naïve-Bayes on the hand, using a probability distribution based on conditional probability predicts in a binary classification,
Yes or No, True or False, or in this dataset, Fraudulent or Not Fraudulent.
So, this is the rationale for selecting these two algorithms. And, since they both are based on different principles, it can give us more
insight into what could be a better-performing algorithm on this dataset or datasets similar to this one.
<br>

<h3>Data Cleaning : </h3>
In order for our algorithms to be implemented there were some data cleaning operations performed in order to prepare it well for further
analysis and classification. For this analysis :
<ul>
  <li>Non-numerical fields were dropped in order to create the feature set and pass it into the ML pipeline. So, records like
  'type', 'nameOrg' and 'nameDest' were dropped from the data table and only numeric data fields were considered. </li>
  <li> Replacing empty records with the average values. First, any empty records for a given field were checked. If there were any,
  then replace it with the mean value of thar field. For binary fields, since, they have 0 and 1, replace them with 1. Even if a transaction
  is not fraudulent but is classified as one, it will increase FPs causing a Type-1 error which will not significantly harm the algorithm.
  But if it is the Type-2 error, i.e. more TNs, then will be very harmful for fraud detection. So, we must work towards minimizing it as much as possible.</li>
</ul>

<br>
<br>
Here are two diagrammatic representations of how the Machine Learning process works and the K-fold cross-validation
technique used for training and testing the algorithm in a much more dynamic way. <br>
<table>
  <tr>
    <td><img src = {{url_for('static', filename='ml_pipeline.png')}} width="550" height="370"></td>
    <td><img src = {{url_for('static', filename='kfold.png')}} width="550" height="370"></td>
  </tr>
</table>

<br>
<br>

<h3>Confusion Matrix : </h3>

<b>Structure of a Confusion Matrix : </b>
<table cellpadding="15">
  <tr>
    <th>True Positives (TPs)</th>
    <th>   False Negative (FNs) </th>
  </tr>
  <tr>
    <th>False Positive (FPs)</th>
    <th>True Negative (TNs) </th>
  </tr>
</table>
<br>
Following are the Confusion Matrices generated for both the Algorithms :
<br>
<b>Logistic Regression : </b>
<table cellpadding="15">
  <tr>
    <th>6362604</th>
    <th>   0   </th>
  </tr>
  <tr>
    <th>   9   </th>
    <th>   7  </th>
  </tr>
</table>
<br>
<b>Gaussian Naïve-Bayes : </b>
<table cellpadding="15">
  <tr>
    <th>4043761</th>
    <th>2318843</th>
  </tr>
  <tr>
    <th>1</th>
    <th>15</th>
  </tr>
</table>
<br>
<h3>Performance Analysis : </h3>

The performance of the two algorithms have been performed based on four performance metrics :
<ul>
  <li> <b>Precision : </b> The precision can be defined as to how
many relevant instances have been retrieved out of all
the retrieved instances. The formula is <i>(TPs)/(TPs + FPs). </i></li>
  <li> <b>Recall : </b> The Recall, sometimes referred to as ’sensitivity’,
can be as the proportion of retrieved instances out
of all relevant instances. The formula is <i>(TPs)/(TPs + FNs) </i></li>
  <li> <b>F1-Score : </b> The F-1 Score provides a unified measure
of both Precision and Recall. It is the harmonic mean of
Precision and Recall and how do both of them, combined,
    measure up for a given classifier. The formula is <i> (2 * Precision * Recall) / (Precision + Recall)</i> </li>
  <li> <b>Accuracy : </b>The Accuracy can be defined as the percentage
of correctly predicted instances out of all the total
    instances. The formula is <i>(TPs + TNs) / (TPs + TNs + FPs + FNs) </i> </li>
</ul>
<br>
Below are the visualizations of our algorithms based on these performance metrics. The first visualization evaluates them for
non-fraudulent instances, i.e. those having the isFraud value '0' where the second visualization evaluates for the fraudulent instances
i.e. those having isFraud as '1'. Based on the isFraud value, what does an ML-algorithm predict in the isFlaggedFraud class? This is what
it most importantly evaluates.
<br>
<table>
  <tr>
    <td><img src = {{url_for('static', filename='eval1.png')}} width="550" height="370"></td>
    <td><img src = {{url_for('static', filename='eval2.png')}} width="550" height="370"></td>
  </tr>
</table>
<br>

<h4>Inferences : </h4>
<ul>
  <li>Logistic Regression performs has a 100% performance on all four metrics for non-fraudulent instances. This could possibly
  happen because of the overwhelming number of '0' values in isFraud.</li>
  <li>Gaussian Naïve-Bayes has a 0% precision and f1-score for fraudulent instances. This means it could detect a fraudulent
  instance as fraudulent on testing. But it could also because of the lack of '1' labelled samples in the dataset. </li>
  <li> So, for the current dataset, Logistic Regression comes out as the better performing algorithm and I have deployed the
  Machine Learning pipeline to production in the /prediction.html page with the Logistic Regression classifier. </li>
</ul>
<br>

<h4>Proposed Enhancements : </h4>
<ul>
  <li> Applying Scalar Transformation to the numeric features like amount, oldBalanceOrg, newBalanceOrg, oldBalanceDest and oldBalanceDest.
   This would enable better feature engineering and in particular, enhance the performance of Gaussian Naïve-Bayes algorithm. </li>
  <li>Once, Scalar Transformation is applied, compute TF-IDF for record and convert each record of a given field of the feature set to a feature vector.
  TD-IDF vectorization helps in realizing the Scalar Transformation at a high level. Since, a scalar is itself a 1-D vector, this would further
  enhance feature engineering and subsequently, lead to better performance of the ML algorithm. </li>
  <li> Lastly, use boolean value for isFraud record. True/False in place of 0/1. It aids in better class labelling and more intuitive classification
  into fraudulent or non-fraudulent instances. </li>
</ul>
<br>
<h3>What more could have been done ? : </h3>
Given some more time to work on this project, a lot of more interesting things could have been done with this dataset. Some of them :
<ul>
  <li> <b>Implement a few more ML algorithms: </b> More algorithms, like Support Vector Machine (SVM) and Boosted Decision Tree could have been
  implemented. I would have loved to evaluate and compare them against our current algorithms. It would have given more insight into what algorithm
    works the best for this dataset. </li>

  <li> <b>Extend Some Deep Learning Functionality : </b> Deep Learning/Neural Nets are more intuitive than Supervised/Unsupervised Machine Learning
  and the performance with data at real-time is also more robust. It is a challenge to implement it efficiently, but I like such challenges and
  would have definitely implemented a Deep Learning pipeline given more time. </li>

  <li><b>Make the UI more interactive : </b> This is more on the front-end level dealing with the HTML pages. Since, that task was more focused
   on Data Analysis with a time constraint, I could not focus much on the UI part. Given more, I would have used some nice interactive styling
  to my HTML pages and make it more graphically appealing at the production level. </li>
</ul>
</body>
